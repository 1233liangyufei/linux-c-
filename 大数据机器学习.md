# 第一章 概述

## 第一节 机器学习定义与应用

大数据机器学习背景知识

机器学习与相关学科的关系

机器学习发展历程

大数据机器学习的主要特征

课程参考书目

具体例子

图片内容分类与识别

物体检测，分割

imagenet

2010 2011 2012 错误率28%-16%

由于采用卷积神经网络的使用，使得性能有了大幅提升

深度学习在学术与产业的飞速发展，CNN快读发展

微软提出的卷积神经网络图像识别已经突破了人力识别

alphago战胜了冠军李世石，使用了深度强化模型，机器学习进入了新的时代

机器学习是20多年的一门多领域交叉学科，涉及概率论，统计学，逼近论，凸分析、算法复杂度理论，让机器自动学习的算法，利用规律对未知行为进行预测，也称为统计学习理论。

例子

- IBM深蓝战胜世界国际象棋大师卡斯帕罗夫
- 语音转换
- 3D体感游戏
- 生物信息
- 机器人
- 自动驾驶
- 量化交易
- 再现古代陶瓷工艺

## 第二节 机器学习与人工智能的关系

人工智能

为机器赋予视觉听觉触觉推理等，如专家系统就是一个例子

机器学习

人工智能计算方法

逻辑斯蒂回归，svm，决策树

表示学习

如何得到一个好的表示浅层自编码机

深度学习

大数据机器学习  多层感知机，卷积神经网络

## 第三节 深度学习方法和其它人工智能方法的共性和差异

基于规则的系统 没有可学习的模块，通过手工设置的程序直接获得

基于表示的学习

- 非深度学习方法，学习到特征映射
- 深度学习，学习得到简单的特征，通过更多的层获得更加抽象的层，再映射出结果

## 第四节 机器学习和数据挖掘的关系

数据挖掘=机器学习+数据库

数据挖掘使用的方法是机器学习一个子集，此外还有强化学习等

## 第五节 机器学习与统计学习的关系

机器学习=统计学习-模型假设检验

机器学习 在线学习 流形学习 主动学习

统计学 空间分析 生成分析 多测试

## 第六节 机器学习的发展里程

人工智能 1955年 达特茅斯会议

机器学习 1980 CMU第一次机器学习国际会议

第一个模型 感知机

1986年 决策树

1995 韦普尼克 SVM机器学习又热门

随机森林 adaboost 大尺度感知机

人工智能的寒冬

hiton andrew leka

## 第七节 大数据机器学习的主要特点

大数据时代，数据量很多

腾讯月活 8 亿，人机关系链1000亿

数据集的大小

数据集规模越来越大

研究人员可以利用数据集进行数据学习

神经元网络的规模越来越打

感知机

后向传播网络

循环神经网络

卷积神经网络

模型性能不断提升

深度学习识别精确度不断提高，前5错误率已经低于3.6

语音识别再利用深度学习后错误率减低了一倍

强化学习 alphagpo 机器可以自己玩，左右互搏

gpu的出现，nvaid 图形卡，提出了GPU图形处理器，现在提出了深度学习显卡特斯拉，股价飞速增长

谷歌公布了TPU技术，张量处理技术

TPU芯片，现有性能非常好，专门为深度学习应用，能耗更小

框架

tensflow 产业

pytorch学术

caffe学术

cntk

keras

mxnet

五本参考书

统计学习方法

深度学习

模式识别和机器学习

机器学习实战

机器学习

# 第二章 机器学习基本概念

## 第一节 基本术语

提纲

基本数据

监督学习

假设空间学习三要素

剃刀原则

没有免费午餐

训练误差与测试误差

正则化

泛化能力

生成模型与判别模型

**数据集**

实例 样本 属性 特征 属性值

属性空间 样本空间

特征向量

数据集，d维样本空间的一个特征向量

机器学习任务

- 分类 人脸识别
- 回归 连续值 房价预测
- 二分类 是否问题 股票涨跌 是否有病
- 多分类任务
- 聚类 文本数据集聚类
- 多标签任务 图像识别  图像检索

## 第二节 监督学习

目的 输入到输出的映射，学习一个模型

模型的集合就是假设空间

模型：

- 概论模型：条件概论分布P(Y|X)
- 非概论模型：决策函数Y=f(X)

联合概论分布：假设输入输出的随机变量X与Y遵循联合概论分布P(X,Y)

训练数据与测试数据

学习系统 得到模型 预测系统

## 第三节 假设空间

学习过程 搜索所有假设空间，与训练集匹配

假设空间搜索：自顶向下，自顶向上

存在一个与训练集一致的空间成为版本空间

## 第四节 学习方法三要素

方法=模型+策略+算法

策略

损失函数和风险函数

损失函数 1次预测的好坏

风险函数 平均意义模型预测的好坏

损失函数

01损失 平方损失 绝对损失 对数损失 对数似然函数

损失函数的期望就是风险函数

经验风险 对训练样本集的损失风险

策略

经验风险最小化与结构风险最小化

样本过小时候，经验函数最小化学习效果未必好，会产生过拟合

防止过拟合，加入了正则化项，罚项，构成结构风险最小化

难点 全局最优 高效求解

机器学习没有显示的解，而要使用数值计算方法寻找，如何寻找至关重要。

## 第五节 奥卡姆剃刀

如无必要，勿增实体

## 第六节 没有免费午餐

哪个模型更加简单

一个算法在某个问题是a比b好，那么在其他情况下b比a好

训练集外误差

二分类问题：总误差与学习算法无关

NFL定理

所有问题出现问题必须相同，或者不同的重要性

假设真实函数f均与分布

脱离具体问题，空谈什么方法好是毫无意义的

## 第七节 训练误差与测试误差

训练误差，训练数据集的平均损失

测试误差，测试数据的平均损失

损失函数是0-1损失

## 第八节 过拟合与模型选择

多项曲线拟合

过拟合现象 多项式过于复杂时候，测试集的损失反而升高了，过拟合

对训练集拟合很高，测试集测试很差

模型不宜过于复杂，会过拟合

模型较为复杂时候，增加训练样本集来避免过拟合

正则化项，来避免过拟合问题

加入了正则化项时候，模型拟合的更好

模型泛化能力，对真实数据的拟合程度，通过测试集来评价

## 第九节 泛化能力

泛化误差

泛化误差上界

性质：样本容量增加，泛化误差区域0，空间容量越大，泛化误差越大

## 第九节 生成模型与判别模型

生成模型

生成方法：朴素贝叶斯，隐马尔科夫模型

判别模型：K近邻，感知机，决策树，logic回归

生成模型：

- 还原联合概论，判别模型不行
- 学习收敛快，样本规模怕增加，收敛更快
- 存在隐变量，使用生成模型，判别模型不行

判别模型

- 直接学习决策函数或条件概论，学习准确率高
- 对数据进行抽象，定义特征与使用特征，可以简化学习问题

# 第三章 模型性能评估

大纲

模型误差评估

- 训练集 训练模型
- 验证集 模型选择，参数选择
- 测试集 模型泛化误差的近似

训练集和测试集的产生

- 留出法 直接划分为互斥
- 交叉验证法
- 自助法

PR曲线 ROC曲线

假设检验 p检验



## 第一节 留出法

划分为互斥两个集合为训练集与测试集

注意

- 注意一致性，避免格外古刹
- 多种划分分割，若干测随机划分进行重复实验

存在问题

- 测试集合太小，评价不准
- 训练集太小，学习程度不够

## 第二节 交叉验证法

k个大小相等互斥子集

k-1个子集训练，一个测试

k此训练，取平均得到最终结果

## 第三节 自助法

每次随机挑选一个样本拷贝放入新数据集，再放回，使得下次仍然可能被采集到，得到了新的测试集

优点

- 适用于数据集小，难以划分
- 数据集产生不同训练集，适用于集成学习

缺点

- 产生训练集改变了初始数据集的分布，引起估计偏差

## 第四节 模型泛化能力性能度量

- 不同任务，性能度量不同
- 回归任务-均方误差

错误率与精度 分类任务常用

错误率在分错数占总数的比率

精度就是1-错误率

查准率 查全率 F1

查准率 P =TP/(TP+FP)

查全率 R= TP/(TP+FN) 召回率，找回多少橙子

二分类混淆矩阵

真正例TP

真反例TN

假正例FP

假反例FN

true false positive negetive

TP 预测橘子，结果也是橘子

TN预测橙子，结果也是橙子

两种方法都有它的缺点



## 第五节 PR曲线

查准率 查全率图

一个模型能完全包住另一个，则更优

平衡点是二者相等的点，比较平衡点也行

不同要求对于查准查全要求不同

F1度量 考虑了二者的权重

多个二分混淆矩阵

- 多次训练
- 多个数据集训练
- 多个分类任务

宏查准率/宏查全率：取平均

微查准/查全：对每个TP/TN分别取平均，然后计算查准查全率

第六节 ROA曲线AUC曲线

纵轴：真正利率 TPR 查全率

横轴：假正例率FPR

预测结果先排序，分类阈值先设置大，再减小

## 第六节 代价敏感错误率

二分类任务 代价矩阵

根据领域知识

代价曲线

代价曲线面积就是学习器期望总体代价

## 第七节 假设检验

性能度量

比较检验

问题提出：

能否直接用上述方法评估性能

原因

- 实验评估的是测试集性能
- 测试集性能与测试集选择有关，测试集不同影响结果
- 机器学习算法有随机性

方案：假设检验

- 测试集上观察到学习器A比B好，则A的泛化性能能都在统计意义上优于B

假设检验

单个学习器检验

- 二项检验
- t检验

不同学习器

- 成对检验法

二项检验

- 假设检验，假设是对学习器泛化错误率分布的某种猜想

## 第八节 t检验

- 多次重复测试
- K个测试错误率

交叉验证t检验

成对t检验

分别t检验，计算均值与方差

## 第九节 偏差与方差

偏差-方差分解

对测试样本

回归方法期望预测

噪音

期望输出与真实标记的偏差

泛化误差 方差 偏差 噪音

泛化性能 

偏差-方差

训练不足，拟合不足，偏差主导

训练加深，拟合增强，方差主导

# 第四章 感知机

## 第一节 感知机模型

罗森布莱特

- 神经网络基础
- 支持向量机模型基础

神经网络，每个神经元就是一个感知机，再加上一个激活函数，使得函数可以连续求导

SVM基础

线性可分与对偶性

感知机

- 输入为实例特征向量，输出实例+1，-1
- 感知机对应于输入空间将实例划分2为正负两类的分离超平面，属于判别模型
- 导入基于误分类的损失函数
- 利用梯度下降对损失函数进行极小化
- 感知机具有简单和易于实现的优点
- 有原始形式与对偶形式

定义（感知机）

假设输入空间是X,输出空间y=（-1，+1）

输入X表示实例的特征向量，对应于输入空间（特征空间）的点，输入表示实例的类别，由输入空间到输出空间的函数：

f(x)=sign(w.x+b)称为感知机

模型参数 w x 内积 权值向量

符号函数 sign(x)，x>=0,+1,x<0,-1

感知机几何解释

线性方程 wx+b=0

对应于超平面S w法向量 b截距 分离正负

分离超平面

## 第二节 感知机学习策略

定义损失函数

自然选择

- 误分类点的数目

另一选择

- 误分类点到超平面的总距离

损失函数

L(w,b)=- yi(w.xi+b)

### 第三节 感知机学习算法

求解最优化问题，损失函数最小

随机梯度下降法

首先任意选择一个超平面，w,b不断极小化目标函数，损损失函数的梯度

感知机学习算法原始形式

输入 训练数据集

输出 感知机模型

- 选择初值 w b
- 训练集选取 xi,yi
- 如果<=0,更新之
- 转至2知道没有误分类点

感知机学习算法

：

算法收敛性：证明 有限次迭代后可以得到一个将数据集完全正确的分离超平面感知机模型

该定理表明

- 误分类次数k有上限的，训练集线性可分时候，感知机原始形式迭代是收敛的
- 感知机存在许多解，依赖初始值，依赖于误分类点选择顺序
- 为得到唯一分离超平面，需要增加约束，如svm
- 线性不可分数据集，迭代震荡

