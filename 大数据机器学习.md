# 第一章 概述

## 第一节 机器学习定义与应用

大数据机器学习背景知识

机器学习与相关学科的关系

机器学习发展历程

大数据机器学习的主要特征

课程参考书目

具体例子

图片内容分类与识别

物体检测，分割

imagenet

2010 2011 2012 错误率28%-16%

由于采用卷积神经网络的使用，使得性能有了大幅提升

深度学习在学术与产业的飞速发展，CNN快读发展

微软提出的卷积神经网络图像识别已经突破了人力识别

alphago战胜了冠军李世石，使用了深度强化模型，机器学习进入了新的时代

机器学习是20多年的一门多领域交叉学科，涉及概率论，统计学，逼近论，凸分析、算法复杂度理论，让机器自动学习的算法，利用规律对未知行为进行预测，也称为统计学习理论。

例子

- IBM深蓝战胜世界国际象棋大师卡斯帕罗夫
- 语音转换
- 3D体感游戏
- 生物信息
- 机器人
- 自动驾驶
- 量化交易
- 再现古代陶瓷工艺

## 第二节 机器学习与人工智能的关系

人工智能

为机器赋予视觉听觉触觉推理等，如专家系统就是一个例子

机器学习

人工智能计算方法

逻辑斯蒂回归，svm，决策树

表示学习

如何得到一个好的表示浅层自编码机

深度学习

大数据机器学习  多层感知机，卷积神经网络

## 第三节 深度学习方法和其它人工智能方法的共性和差异

基于规则的系统 没有可学习的模块，通过手工设置的程序直接获得

基于表示的学习

- 非深度学习方法，学习到特征映射
- 深度学习，学习得到简单的特征，通过更多的层获得更加抽象的层，再映射出结果

## 第四节 机器学习和数据挖掘的关系

数据挖掘=机器学习+数据库

数据挖掘使用的方法是机器学习一个子集，此外还有强化学习等

## 第五节 机器学习与统计学习的关系

机器学习=统计学习-模型假设检验

机器学习 在线学习 流形学习 主动学习

统计学 空间分析 生成分析 多测试

## 第六节 机器学习的发展里程

人工智能 1955年 达特茅斯会议

机器学习 1980 CMU第一次机器学习国际会议

第一个模型 感知机

1986年 决策树

1995 韦普尼克 SVM机器学习又热门

随机森林 adaboost 大尺度感知机

人工智能的寒冬

hiton andrew leka

## 第七节 大数据机器学习的主要特点

大数据时代，数据量很多

腾讯月活 8 亿，人机关系链1000亿

数据集的大小

数据集规模越来越大

研究人员可以利用数据集进行数据学习

神经元网络的规模越来越打

感知机

后向传播网络

循环神经网络

卷积神经网络

模型性能不断提升

深度学习识别精确度不断提高，前5错误率已经低于3.6

语音识别再利用深度学习后错误率减低了一倍

强化学习 alphagpo 机器可以自己玩，左右互搏

gpu的出现，nvaid 图形卡，提出了GPU图形处理器，现在提出了深度学习显卡特斯拉，股价飞速增长

谷歌公布了TPU技术，张量处理技术

TPU芯片，现有性能非常好，专门为深度学习应用，能耗更小

框架

tensflow 产业

pytorch学术

caffe学术

cntk

keras

mxnet

五本参考书

统计学习方法

深度学习

模式识别和机器学习

机器学习实战

机器学习

# 第二章 机器学习基本概念

## 第一节 基本术语

提纲

基本数据

监督学习

假设空间学习三要素

剃刀原则

没有免费午餐

训练误差与测试误差

正则化

泛化能力

生成模型与判别模型

**数据集**

实例 样本 属性 特征 属性值

属性空间 样本空间

特征向量

数据集，d维样本空间的一个特征向量

机器学习任务

- 分类 人脸识别
- 回归 连续值 房价预测
- 二分类 是否问题 股票涨跌 是否有病
- 多分类任务
- 聚类 文本数据集聚类
- 多标签任务 图像识别  图像检索

## 第二节 监督学习

目的 输入到输出的映射，学习一个模型

模型的集合就是假设空间

模型：

- 概论模型：条件概论分布P(Y|X)
- 非概论模型：决策函数Y=f(X)

联合概论分布：假设输入输出的随机变量X与Y遵循联合概论分布P(X,Y)

训练数据与测试数据

学习系统 得到模型 预测系统

## 第三节 假设空间

学习过程 搜索所有假设空间，与训练集匹配

假设空间搜索：自顶向下，自顶向上

存在一个与训练集一致的空间成为版本空间

## 第四节 学习方法三要素

方法=模型+策略+算法

策略

损失函数和风险函数

损失函数 1次预测的好坏

风险函数 平均意义模型预测的好坏

损失函数

01损失 平方损失 绝对损失 对数损失 对数似然函数

损失函数的期望就是风险函数

经验风险 对训练样本集的损失风险

策略

经验风险最小化与结构风险最小化

样本过小时候，经验函数最小化学习效果未必好，会产生过拟合

防止过拟合，加入了正则化项，罚项，构成结构风险最小化

难点 全局最优 高效求解

机器学习没有显示的解，而要使用数值计算方法寻找，如何寻找至关重要。

## 第五节 奥卡姆剃刀

如无必要，勿增实体

## 第六节 没有免费午餐

哪个模型更加简单

一个算法在某个问题是a比b好，那么在其他情况下b比a好

训练集外误差

二分类问题：总误差与学习算法无关

NFL定理

所有问题出现问题必须相同，或者不同的重要性

假设真实函数f均与分布

脱离具体问题，空谈什么方法好是毫无意义的

## 第七节 训练误差与测试误差

训练误差，训练数据集的平均损失

测试误差，测试数据的平均损失

损失函数是0-1损失

## 第八节 过拟合与模型选择

多项曲线拟合

过拟合现象 多项式过于复杂时候，测试集的损失反而升高了，过拟合

对训练集拟合很高，测试集测试很差

模型不宜过于复杂，会过拟合

模型较为复杂时候，增加训练样本集来避免过拟合

正则化项，来避免过拟合问题

加入了正则化项时候，模型拟合的更好

模型泛化能力，对真实数据的拟合程度，通过测试集来评价

## 第九节 泛化能力

泛化误差

泛化误差上界

性质：样本容量增加，泛化误差区域0，空间容量越大，泛化误差越大

## 第九节 生成模型与判别模型

生成模型

生成方法：朴素贝叶斯，隐马尔科夫模型

判别模型：K近邻，感知机，决策树，logic回归

生成模型：

- 还原联合概论，判别模型不行
- 学习收敛快，样本规模怕增加，收敛更快
- 存在隐变量，使用生成模型，判别模型不行

判别模型

- 直接学习决策函数或条件概论，学习准确率高
- 对数据进行抽象，定义特征与使用特征，可以简化学习问题

